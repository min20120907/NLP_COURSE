{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um8Z-HYBexDi"
   },
   "source": [
    "# Exercise 5: Word2Vec\n",
    "\n",
    "In this exercise, you will implement skip-gram Word2Vec with negative sampling.\n",
    "\n",
    "You should complete the parts of the exercies that are marked as **TODO**.\n",
    "A correctly completed **TODO** gives 2 bonus points. Partially correct answers give 1 bonus point.\n",
    "Some **TODO**s are inside a comment in a code block: Here, you should complete the line of code.\n",
    "Other **TODO**s are inside text blocks: Here, you should write a few sentences to answer the question, preferably inside the same text block.\n",
    "\n",
    "**Submission deadline:** 30.11.2021\n",
    "\n",
    "**Instructions for submission:** After completing the exercise, save a copy of the notebook as exercise5_word2vec_MATRIKELNUMMER.ipynb, where MATRIKELNUMMER is your student ID number. Then upload the notebook to moodle (submission exercise sheet 5).\n",
    "\n",
    "In order to understand the code, it can be helpful to experiment a bit during development. You are welcome to print variables, reduce dataset sizes, or change the hyperparameters. But please remove these changes before submitting the notebook. If we cannot run your notebook (e.g., because you commented out an important variable, or because a print statement is congesting stdout), then we cannot grade it. \n",
    "\n",
    "To make the most of this exercise, you should try to read and understand the entire code, not just the parts that contain a **TODO**. If you have questions, write them down for the exercise, which will happen in the week after the submission deadline.\n",
    "\n",
    "**Important:** This is not a pytorch exercise. So if you are using colab, there is no need to switch to a GPU runtime. Pytorch exercises start next week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODf2A5HRKLJF"
   },
   "source": [
    "# Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "Pa0dYqRYKLsq",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/min20120907/.local/lib/python3.8/site-packages (1.20.2)\n",
      "Requirement already satisfied: tqdm in /home/min20120907/.local/lib/python3.8/site-packages (4.60.0)\n",
      "Requirement already satisfied: nltk in /home/min20120907/.local/lib/python3.8/site-packages (3.6.1)\n",
      "Requirement already satisfied: click in /usr/lib/python3/dist-packages (from nltk) (7.0)\n",
      "Requirement already satisfied: regex in /home/min20120907/.local/lib/python3.8/site-packages (from nltk) (2021.4.4)\n",
      "Requirement already satisfied: joblib in /home/min20120907/.local/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
      "Requirement already satisfied: tqdm in /home/min20120907/.local/lib/python3.8/site-packages (from nltk) (4.60.0)\n",
      "Requirement already satisfied: gensim==3.8.3 in /home/min20120907/.local/lib/python3.8/site-packages (3.8.3)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /home/min20120907/.local/lib/python3.8/site-packages (from gensim==3.8.3) (1.20.2)\n",
      "Requirement already satisfied: six>=1.5.0 in /usr/lib/python3/dist-packages (from gensim==3.8.3) (1.14.0)\n",
      "Requirement already satisfied: scipy>=0.18.1 in /home/min20120907/.local/lib/python3.8/site-packages (from gensim==3.8.3) (1.6.2)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /home/min20120907/.local/lib/python3.8/site-packages (from gensim==3.8.3) (5.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install tqdm\n",
    "!pip install nltk\n",
    "!pip install gensim==3.8.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QRFXB5yTNO2p"
   },
   "source": [
    "## Implementation in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "WNWK1wAcVbf0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import nltk\n",
    "from tqdm.notebook import trange\n",
    "from collections import Counter\n",
    "from itertools import cycle, takewhile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "03Bk8qkQBaCT"
   },
   "source": [
    "We use the NLTK Gutenberg corpus as unlabeled training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "L3WUxxvTVQ-U"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /home/min20120907/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/min20120907/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('gutenberg')\n",
    "nltk.download('punkt')\n",
    "corpus = nltk.corpus.gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx69xoYTeo42"
   },
   "source": [
    "Hyperparameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "1TuEJ5DH5pWj"
   },
   "outputs": [],
   "source": [
    "NUM_ITER = 2\n",
    "WINDOW_SIZE = 2\n",
    "NUM_NEGATIVES = 5\n",
    "LR = 0.025\n",
    "MIN_COUNT = 25\n",
    "VECTOR_SIZE = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ImU1ikgAny3I"
   },
   "source": [
    "We count word frequencies, so that we can later sample negatives according to their frequency (see below).\n",
    "We only keep words that occur at least MIN_COUNT times.\n",
    "idx_to_word and word_to_idx allow us to map from words (strings) to vocabulary indices (integers) and vice versa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "8DubXDEUGv8t"
   },
   "outputs": [],
   "source": [
    "word_to_count_raw = Counter(corpus.words())\n",
    "word_to_count = Counter({word: count for word, count \\\n",
    "                         in word_to_count_raw.items() if count >= MIN_COUNT})\n",
    "\n",
    "idx_to_word = [word for word, count in word_to_count.most_common()]\n",
    "word_to_idx = {word: idx for idx, word in enumerate(idx_to_word)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4vltCRModaz"
   },
   "source": [
    "During training, this generator function will iterate over word pairs in the corpus.\n",
    "For the skip-gram algorithm, a word pair is defined as follows:\n",
    "The first word (focus word) is in the middle of the window.\n",
    "The second word (context word) is at least one and at most WINDOW_SIZE steps away from the focus word.\n",
    "Every focus-context pair becomes a separate input sample.\n",
    "You should skip unknown (UNK) words (index -1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "r3SfjOwaJgbp"
   },
   "outputs": [],
   "source": [
    "def make_positive_generator(sentences, window_size):\n",
    "  '''\n",
    "  Build a generator that yields positive focus-context word pairs\n",
    "\n",
    "  @param sentences List of lists of vocabulary indices (None for unknown words)\n",
    "  @param window_size Size of context window (per direction)\n",
    "  '''\n",
    "\n",
    "  sentences = list(sentences)\n",
    "  while True:\n",
    "    np.random.shuffle(sentences)\n",
    "    count = 0\n",
    "\n",
    "    for sentence in sentences:\n",
    "      non_unk_positions = np.arange(sentence.shape[0])[sentence != -1]\n",
    "      \n",
    "      for focus_position in non_unk_positions:\n",
    "        context_positions = []\n",
    "        for a in focus_position:\n",
    "            if a in non_unk_positions:\n",
    "                context_positions.append(a)\n",
    "        context_position = np.array(context_position)\n",
    "        for context_position in context_positions:\n",
    "          yield count, sentence[[focus_position, context_position]]\n",
    "          count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnBBQAaOo5Rh"
   },
   "source": [
    "This generator will produce random context words as negative samples.\n",
    "We will sample words according to their frequency in the corpus (stored in word_to_count). You should clip the absolute frequency at 100. This will result in under-sampling of very frequent words (e.g., stop words like 'the', 'are'...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "3u8emcl9R_q_"
   },
   "outputs": [],
   "source": [
    "def make_negative_generator(word_to_count, num_negatives):\n",
    "  '''\n",
    "  Build a generator that yields vectors of randomly sampled negatives.\n",
    "\n",
    "  @param word_to_count Counter object that maps words to absolute frequencies\n",
    "  @param num_negatives How many negatives to return at every step\n",
    "  '''\n",
    "  freqs_table = np.array([count for _, count in word_to_count.most_common()])\n",
    "  freqs_table_clipped = np.clip(freqs_table, 100) # TODO: Clip absolute frequency at 100\n",
    "  probas_table = freqs_table_clipped.astype(float) / freqs_table_clipped.sum()\n",
    "  \n",
    "  count = 0\n",
    "  while True:\n",
    "    for negatives in np.random.choice(np.arange(probas_table.shape[0]),\n",
    "                                      p=probas_table,\n",
    "                                      size=(10000, num_negatives)):\n",
    "      yield count, negatives \n",
    "      count += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DW8ASkb7-7UQ"
   },
   "source": [
    "Negative sampling replaces the softmax function with a series of sigmoids.\n",
    "You should implement the sigmoid function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "iz1cLJU8ULzW"
   },
   "outputs": [],
   "source": [
    "def sigmoid(logits):\n",
    "  '''\n",
    "  Applies sigmoid function to a tensor of logits.\n",
    "\n",
    "  @param logits Tensor that contains real-valued logits.\n",
    "  '''\n",
    "  probas = 1 / (1 + math.exp(-logits)) # TODO: Use the sigmoid function to turn logits into probabilties\n",
    "  return probas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v49eBDEKbUoC"
   },
   "source": [
    "This function initializes the word vectors and context vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "i5Htb7_8kN5o"
   },
   "outputs": [],
   "source": [
    "def init_weights(num_vectors, vector_size):\n",
    "  '''\n",
    "  Initialize matrix of word (focus) vectors and matrix of context vectors.\n",
    "\n",
    "  @param num_vectors Number of vectors (vocabulary size)\n",
    "  @param vector_size Vector size (dimensionality)\n",
    "  '''\n",
    "\n",
    "  limit = 0.5 / vector_size\n",
    "  W = np.random.uniform(-limit, limit, size=(num_vectors, vector_size))\n",
    "  V = np.zeros_like(W)\n",
    "  return W, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c5OU5JOyAM-x"
   },
   "source": [
    "The following function does a single parameter update for one focus word (with one positive and N negative context words).\n",
    "You should complete the indicated lines of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "H7DLu2AJC8hJ"
   },
   "outputs": [],
   "source": [
    "def training_step(W, V, positive_generator, negative_generator, lr):\n",
    "  '''\n",
    "  Parameter update for 1 focus word, 1 positive and N negative context words.\n",
    "\n",
    "  @param W Matrix of word (focus) vectors\n",
    "  @param V Matrix of context vectors\n",
    "  @param positive_generator Generates focus word and positive context word\n",
    "  @param negative_generator Generates N randomly sampled words\n",
    "  @param lr Current learning rate\n",
    "  '''\n",
    "\n",
    "  _, (focus_word, positive) = next(positive_generator)\n",
    "  _, negatives = next(negative_generator)\n",
    "\n",
    "  context_words = np.concatenate([np.array([positive]), negatives], 0)\n",
    "  \n",
    "  logits = W[focus_word].dot(V[context_words].T) \n",
    "  logits_clipped = np.clip(logits, -6, 6) # clip logits to avoid infinity in exp\n",
    "  probas = sigmoid(logits_clipped) # vector of predicted probabilities\n",
    "\n",
    "  labels = [] # TODO: Produce the correct labels for the context words\n",
    "  for a in range(len(W)):\n",
    "    if W[a] > 0:\n",
    "        labels.append(1)\n",
    "    else:\n",
    "        labels.append(0)\n",
    "  # Hint: Look at the order in which we concatenated the positive and negative context words\n",
    "\n",
    "  for context_word, proba, label in zip(context_words, probas, labels):\n",
    "    # nll_loss = -np.log(proba) * label - np.log(1 - proba) * (1 - label)\n",
    "    # This is the definition of the loss. We don't have to compute the loss\n",
    "    # explicitly, just its gradients, so it is commented out here.\n",
    "\n",
    "    gradient_v = null_loss(log_softmax(context_word)) # TODO: calculate the gradient of nll_loss w.r.t. the v vector of the context word\n",
    "    gradient_w = null_loss(log_softmax(focus_word)) # TODO: calculate the gradient of nll_loss w.r.t. the w vector of the focus word\n",
    "\n",
    "    V[context_word, :] -= lr * gradient_v\n",
    "    W[focus_word, :] -= lr * gradient_w"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CHZH588aHUVy"
   },
   "source": [
    "Now we train the model. This will take a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "RCaW0rGJYcBu"
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.int64' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_173600/2147957049.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtakewhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0msteps\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcount_steps_per_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_generator\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mNUM_ITER\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mlr_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mLR\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0msteps\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_173600/2147957049.py\u001b[0m in \u001b[0;36mcount_steps_per_iter\u001b[0;34m(positive_generator)\u001b[0m\n\u001b[1;32m     16\u001b[0m   '''\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m   \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpositive_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtakewhile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mc0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_generator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_173600/30048202.py\u001b[0m in \u001b[0;36mmake_positive_generator\u001b[0;34m(sentences, window_size)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mfocus_position\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnon_unk_positions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mcontext_positions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfocus_position\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcontext_position\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcontext_positions\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'numpy.int64' object is not iterable"
     ]
    }
   ],
   "source": [
    "W, V = init_weights(len(word_to_idx), VECTOR_SIZE)\n",
    "\n",
    "sentences = [np.array([word_to_idx.get(word, -1) for word in sentence]) \\\n",
    "             for sentence in corpus.sents()]\n",
    "\n",
    "positive_generator = make_positive_generator(sentences, \n",
    "                                             window_size=WINDOW_SIZE)\n",
    "negative_generator = make_negative_generator(word_to_count, \n",
    "                                             num_negatives=NUM_NEGATIVES)\n",
    "\n",
    "def count_steps_per_iter(positive_generator):\n",
    "  '''\n",
    "  Counts the number of steps until generator sample counter restarts.\n",
    "\n",
    "  @param positive_generator Generator that generates count, value pairs\n",
    "  '''\n",
    "  \n",
    "  c0, _ = next(positive_generator)\n",
    "  return 1 + len(list(takewhile(lambda x: x[0] != c0, positive_generator)))\n",
    "\n",
    "steps = count_steps_per_iter(positive_generator) * NUM_ITER\n",
    "lr_decay = (LR / steps * (steps-i) for i in range(steps))\n",
    "\n",
    "for _ in trange(steps):\n",
    "  training_step(W, V, positive_generator, negative_generator, next(lr_decay))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_93JTKL9hA-k"
   },
   "source": [
    "A nice property of word vectors is that they allow us to measure word similarity as cosine similarity.\n",
    "You should complete the function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ns7eO9p4dKEc"
   },
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1, vec2):\n",
    "  similarity = (sqrt(vec1[0]**2+vec2[0]**2), sqrt(vec1[0]**2+vec2[0]**2)) # TODO: Calculate the cosine similarity of vec1 and vec2\n",
    "  return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5Fgj60IqCjp"
   },
   "source": [
    "Now we can look at the neighbors of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1ILH4LSup19"
   },
   "outputs": [],
   "source": [
    "WORDS = ('eye', 'time', 'Emma', 'young', 'yellow', 'laugh', 'exist')\n",
    "\n",
    "for word in WORDS:\n",
    "  print('word:', word)\n",
    "  word_vec = W[word_to_idx[word]]\n",
    "  print('vector:', word_vec[:5], '...')\n",
    "  sims = np.array([cosine_similarity(word_vec, other_vec) for other_vec in W])\n",
    "  top5 = sims.argsort()[::-1][1:6] # top1 neighbor is same word\n",
    "  print('top5 neighbors:', \n",
    "        ' '.join([idx_to_word[idx] + ' ' + str(sims[idx]) for idx in top5]))\n",
    "  print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ji5TmISih6S"
   },
   "source": [
    "## gensim\n",
    "\n",
    "Usually, you would not implement Word2Vec from scratch but use a library, such as gensim.\n",
    "The implementation in gensim runs a lot faster than ours, and it uses some additional tricks to train better embeddings.\n",
    "You can find the documentation at  https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "WIX4zE6dX88t"
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "word2vec = Word2Vec(corpus.sents(), \n",
    "                    size=VECTOR_SIZE,\n",
    "                    iter=NUM_ITER,\n",
    "                    min_count=MIN_COUNT,\n",
    "                    negative=NUM_NEGATIVES,\n",
    "                    hs=0, # no hierarchical softmax means negative sampling\n",
    "                    sg=1) # skip-gram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YjqYS8M9jOUD"
   },
   "source": [
    "Besides being fast to train, gensim makes vector lookup and nearest neighbor search really easy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KZAJfdD6nTvs"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'WORDS' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_173600/4124167698.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mWORDS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'word:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mword_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vector:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mword_vec\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m   \u001b[0mtop5\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword2vec\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WORDS' is not defined"
     ]
    }
   ],
   "source": [
    "for word in WORDS:\n",
    "  print('word:', word)\n",
    "  word_vector = word2vec.wv[word]\n",
    "  print('vector:', word_vec[:5], '...')\n",
    "  top5 = word2vec.wv.most_similar(word)[:5]\n",
    "  print('top5 neighbors:', \n",
    "        ' '.join([word + ' ' + str(sim) for word, sim in top5]))\n",
    "  print('-'*50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_sLxvh6zPKp2"
   },
   "source": [
    "## FastText\n",
    "\n",
    "An issue with Word2Vec is that it cannot assign vectors to out-of-vocabulary words. Out-of-vocabulary words happen when a word falls under the MIN_COUNT threshold, or if it does not appear in the training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vs87ISB4nRDM"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "  print(word2vec.wv['laughable'])\n",
    "except Exception as e:\n",
    "  print('ERROR', e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QCvuXO9mqvS7"
   },
   "source": [
    "FastText is an extension of Word2Vec that addresses this problem. Here, you should train FastText with the same corpus and parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0s0LUl3GnpVM"
   },
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "fasttext = None # TODO: Train FastText with the same corpus and parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bjLvgcyWq5RN"
   },
   "source": [
    "Now, the word 'laughable' gets a vector, even though it is not in the model vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Soi3JWOMn5ZW"
   },
   "outputs": [],
   "source": [
    "print(fasttext.wv['laughable'])\n",
    "print('laughable' in fasttext.wv.vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V0prK9fuPZmg"
   },
   "source": [
    "**TODO:** In a few sentences, explain how FastText is able to assign vectors to out-of-vocabulary words."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercise5_word2vec_todo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
