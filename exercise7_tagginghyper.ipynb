{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "um8Z-HYBexDi"
   },
   "source": [
    "# Exercise 7: Tagging and hyperparameter optimization\n",
    "\n",
    "In this exercise, you will implement and train a bidirectional GRU for Part-Of-Speech tagging. You will also do some hyperparameter tuning.\n",
    "\n",
    "You should complete the parts of the exercise that are marked as **TODO**.\n",
    "A correctly completed **TODO** gives 2 bonus points. Partially correct answers give 1 bonus point.\n",
    "Some **TODO**s are inside a comment in a code block: Here, you should complete the line of code.\n",
    "Other **TODO**s are inside a text block: Here, you should write a few sentences to answer the question.\n",
    "\n",
    "**Important:** Some students were under the impression that you have to complete a TODO in a _single_ line of code. That is not the case, you can use as many lines as you need.\n",
    "\n",
    "**Submission deadline:** 14.12.2021\n",
    "\n",
    "**Instructions for submission:** After completing the exercise, save a copy of the notebook as exercise7_tagginghyper_MATRIKELNUMMER.ipynb, where MATRIKELNUMMER is your student ID number. Then upload the notebook to moodle (submission exercise sheet 7).\n",
    "\n",
    "In order to understand the code, it can be helpful to experiment a bit during development, e.g., to print tensors or their shapes. But please remove these changes before submitting the notebook. If we cannot run your notebook, or if a print statement is congesting stdout too much, then we cannot grade it. \n",
    "\n",
    "To make the most of this exercise, you should try to read and understand the entire code, not just the parts that contain a **TODO**. If you have questions, write them down for the exercise, which will happen in the week after the submission deadline.\n",
    "\n",
    "**CUDA:** You can use a GPU for this exercise (on colab: Runtime -> Change Runtime Type -> GPU). This is not mandatory, but it will speed up training epochs, thereby allowing you to test more hyperparameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ODf2A5HRKLJF"
   },
   "source": [
    "# Required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Pa0dYqRYKLsq"
   },
   "outputs": [],
   "source": [
    "!pip install -q numpy==1.18.0\n",
    "!pip install -q torch==1.7.0\n",
    "!pip install -q matplotlib==3.2.2\n",
    "!pip install -q nltk==3.2.5\n",
    "!pip install -q ipywidgets==7.5.1\n",
    "!pip install -q tqdm==4.41.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WNWK1wAcVbf0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "  torch.cuda.manual_seed(0)\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "import nltk\n",
    "import copy\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OvPEpBr6yLrA"
   },
   "source": [
    "# Data\n",
    "\n",
    "We will do Part-Of-Speech tagging, which is the task of assigning Part-Of-Speech tags (e.g., NOUN, VERB) to the words of a sentence.\n",
    "\n",
    "We will use the Wall Street Journal portion of the Penn treebank.\n",
    "State-of-the-Art models can get very high accuracies on this benchmark.\n",
    "With our simple model, we are aiming for a test set accuracy of around 80-85%.\n",
    "\n",
    "We are using the dataset split from https://nlp.stanford.edu/pubs/CICLing2011-manning-tagging.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "N25Z9Bh-HgL4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package treebank to\n",
      "[nltk_data]     /home/min20120907/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/treebank.zip.\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/min20120907/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/universal_tagset.zip.\n"
     ]
    }
   ],
   "source": [
    "WSJ_FILEIDS = {'train': [f'wsj_{i:04d}.mrg' for i in range(1, 19)],\n",
    "               'dev': [f'wsj_{i:04d}.mrg' for i in range(19, 22)],\n",
    "               'test': [f'wsj_{i:04d}.mrg' for i in range(22, 25)]}\n",
    "\n",
    "nltk.download('treebank')\n",
    "nltk.download('universal_tagset')\n",
    "corpus = nltk.corpus.treebank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4BSrwYumyzoU"
   },
   "source": [
    "## Vocabularies\n",
    "\n",
    "Here, we build the word and tag vocabulary. Note that we are not using any pretrained embeddings or models, so we can only learn words that exist in the training set. This will severely limit our test set preformance, but that's okay for the purpose of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yWOEskTJEJ1W"
   },
   "outputs": [],
   "source": [
    "word2idx = {'!PAD!': 0, '!UNK!': 1}\n",
    "tag2idx = {'!PAD!': 0}\n",
    "\n",
    "for word, tag in corpus.tagged_words(WSJ_FILEIDS['train'], tagset='universal'):\n",
    "  word2idx[word] = word2idx.get(word, len(word2idx))\n",
    "  tag2idx[tag] = tag2idx.get(tag, len(tag2idx))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6elGuCIO5P8N"
   },
   "source": [
    "## Encoding and padding\n",
    "\n",
    "Here, we translate the sentences into sequences of word indices (inputs) and tag indices (targets).\n",
    "\n",
    "Since the sequences have different lengths, we must pad the shorter ones with zeros.\n",
    "This ensures that pytorch can put them into matrix format.\n",
    "We will later ignore the padded positions when calculating the loss.\n",
    "\n",
    "You should pad the inputs and targets with zeros, so that they become a matrix of shape $\\mathbb{R}^{N \\times J}$, where $N$ is the number of sentences (datapoints) and $J$ is the length of the longest sentence (sequence length).\n",
    "The padding function has already been imported above. \n",
    "\n",
    "**Important:** Set batch_first=True when using the padding function, otherwise the batch and sequence axes will be reversed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "nOHKeyQ9f7oy"
   },
   "outputs": [],
   "source": [
    "datasets = {}\n",
    "for dsetname, fileids in WSJ_FILEIDS.items():\n",
    "  inputs, targets = [], []\n",
    "  lengthes = []\n",
    "  for i, sent in enumerate(corpus.tagged_sents(fileids, tagset='universal')):\n",
    "    words = [word2idx.get(word, word2idx['!UNK!']) for word, tag in sent]\n",
    "    tags = [tag2idx[tag] for word, tag in sent]\n",
    "    inputs.append(torch.tensor(words))\n",
    "    targets.append(torch.tensor(tags))\n",
    "    lengthes.append(len(words))\n",
    "  # print(max(lengthes))\n",
    "  # N is the number of sentences(datapoints)\n",
    "  # J is the length of the longest sentences(sequence length)\n",
    "  # TODO: Pad inputs and targets with zeros\n",
    "  inputs_padded = pad_sequence(inputs, batch_first=True)\n",
    "  targets_padded = pad_sequence(targets, batch_first=True)\n",
    "  \n",
    "  datasets[dsetname] = data.TensorDataset(inputs_padded, targets_padded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5rn7BGE5DXA"
   },
   "source": [
    "Let's look at some data.\n",
    "We show each datapoint first as a sequence of indices, and then as a sequence of words and tags (word|tag).\n",
    "\n",
    "Since we are not using pretrained embeddings, we have many cases of unknown (!UNK!) words in the dev and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "sOhYuV0S41Cv"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with 203 sentences\n",
      "2|1 3|1 4|2 5|3 6|1 7|4 4|2 8|5 9|5 10|6 11|1 12|7 13|6 14|4 15|1 16|1 17|3 18|2\n",
      "Pierre|NOUN Vinken|NOUN ,|. 61|NUM years|NOUN old|ADJ ,|. will|VERB join|VERB the|DET board|NOUN as|ADP a|DET nonexecutive|ADJ director|NOUN Nov.|NOUN 29|NUM .|.\n",
      "\n",
      "19|1 3|1 20|5 21|1 22|7 23|1 24|1 4|2 10|6 25|1 26|5 27|1 18|2\n",
      "Mr.|NOUN Vinken|NOUN is|VERB chairman|NOUN of|ADP Elsevier|NOUN N.V.|NOUN ,|. the|DET Dutch|NOUN publishing|VERB group|NOUN .|.\n",
      "\n",
      "28|1 29|1 4|2 30|3 6|1 7|4 31|8 32|4 21|1 22|7 33|1 34|1 35|1 36|1 4|2 37|5 38|5 39|9 13|6 14|4 15|1 22|7 40|6 41|4 42|4 43|1 18|2\n",
      "Rudolph|NOUN Agnew|NOUN ,|. 55|NUM years|NOUN old|ADJ and|CONJ former|ADJ chairman|NOUN of|ADP Consolidated|NOUN Gold|NOUN Fields|NOUN PLC|NOUN ,|. was|VERB named|VERB *-1|X a|DET nonexecutive|ADJ director|NOUN of|ADP this|DET British|ADJ industrial|ADJ conglomerate|NOUN .|.\n",
      "\n",
      "dev with 38 sentences\n",
      "1141|1 1263|1 1|1 4|2 1354|3 6|1 7|4 4|2 37|5 38|5 1|9 669|4 690|1 375|1 376|1 31|8 1143|1 1153|5 1144|1 4|2 1|6 125|4 1|1 18|2\n",
      "John|NOUN R.|NOUN !UNK!|NOUN ,|. 49|NUM years|NOUN old|ADJ ,|. was|VERB named|VERB !UNK!|X senior|ADJ executive|NOUN vice|NOUN president|NOUN and|CONJ chief|NOUN operating|VERB officer|NOUN ,|. !UNK!|DET new|ADJ !UNK!|NOUN .|.\n",
      "\n",
      "558|12 8|5 495|5 39|9 50|11 1636|5 50|11 1|1 1|1 4|2 376|1 31|8 1143|1 690|1 1144|1 18|2\n",
      "He|PRON will|VERB continue|VERB *-1|X to|PRT report|VERB to|PRT !UNK!|NOUN !UNK!|NOUN ,|. president|NOUN and|CONJ chief|NOUN executive|NOUN officer|NOUN .|.\n",
      "\n",
      "19|1 1|1 37|5 690|1 375|1 376|1 22|7 40|6 1|1 585|1 203|1 18|2\n",
      "Mr.|NOUN !UNK!|NOUN was|VERB executive|NOUN vice|NOUN president|NOUN of|ADP this|DET !UNK!|NOUN holding|NOUN company|NOUN .|.\n",
      "\n",
      "test with 29 sentences\n",
      "1|2 1|7 105|12 1|1 113|1 4|2 73|1 1|1 1|1 120|1 8|5 1636|5 383|1 22|7 10|6 1339|4 1|1 86|6 1|9 1|5 12|7 1|1 22|7 1|4 1457|1 1|1 18|2 1|2\n",
      "!UNK!|. !UNK!|ADP its|PRON !UNK!|NOUN year|NOUN ,|. The|NOUN !UNK!|NOUN !UNK!|NOUN Journal|NOUN will|VERB report|VERB events|NOUN of|ADP the|DET past|ADJ !UNK!|NOUN that|DET !UNK!|X !UNK!|VERB as|ADP !UNK!|NOUN of|ADP !UNK!|ADJ business|NOUN !UNK!|NOUN .|. !UNK!|.\n",
      "\n",
      "1|3 1|1 1|6 1|9 1|5 10|6 1578|1 22|7 1|4 1|1 111|5 1|5 1|9 104|7 1|3 18|2\n",
      "!UNK!|NUM !UNK!|NOUN !UNK!|DET !UNK!|X !UNK!|VERB the|DET face|NOUN of|ADP !UNK!|ADJ !UNK!|NOUN were|VERB !UNK!|VERB !UNK!|X in|ADP !UNK!|NUM .|.\n",
      "\n",
      "1165|6 113|1 10|6 1|1 1|1 4|2 1|1 1|1 31|8 1|1 1|1 1678|5 50|11 470|1 18|2\n",
      "That|DET year|NOUN the|DET !UNK!|NOUN !UNK!|NOUN ,|. !UNK!|NOUN !UNK!|NOUN and|CONJ !UNK!|NOUN !UNK!|NOUN came|VERB to|PRT market|NOUN .|.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "idx2word = sorted(list(word2idx.keys()), key=lambda word:word2idx[word])\n",
    "idx2tag = sorted(list(tag2idx.keys()), key=lambda tag:tag2idx[tag])\n",
    "\n",
    "for dsetname in ('train', 'dev', 'test'):\n",
    "  print(dsetname, 'with', len(datasets[dsetname]), 'sentences')\n",
    "  for (inputs, targets), _ in zip(datasets[dsetname], range(3)):\n",
    "    inputs, targets = inputs.numpy(), targets.numpy()\n",
    "    print(' '.join(f'{word}|{tag}' \\\n",
    "                   for word, tag in zip(inputs, targets) if word != 0))\n",
    "    print(' '.join(f'{idx2word[word]}|{idx2tag[tag]}'\\\n",
    "                   for word, tag in zip(inputs, targets) if word != 0))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWMVDiYR0lbE"
   },
   "source": [
    "# Model\n",
    "\n",
    "The model consists of four layers.\n",
    "You should instantiate all layers in the init function, using the appropriate hyperparameters from the config dictionary.\n",
    "\n",
    "- A **word embedding lookup layer**, which transforms our inputs into a tensor $\\mathbf{E} \\in \\mathbb{R}^{B \\times J \\times D_{emb}}$, where $B$ is the batch size, $J$ is the sequence length, and $D_{emb}$ is our embedding_size hyperparameter\n",
    "- A **single-layer bidirectional GRU**, which transforms $\\mathbf{E}$ into a tensor $\\mathbf{H} \\in \\mathbb{R}^{B \\times J \\times 2D_{hidden}}$, where $D_{hidden}$ is our hidden_size hyperparameter. The factor $2$ stems from the fact that the GRU is bidirectional. **Important:** When instantiating the GRU layer, you should set batch_first=True, otherwise the layer will expect the sequence axis to come before the batch axis.\n",
    "- A **linear layer**, which transforms $\\mathbf{H}$ into a tensor $\\mathbf{O} \\in \\mathbb{R}^{B \\times J \\times C}$, where $C$ is the number of tags (output classes).\n",
    "- A **dropout layer**, which zeros out neurons with probability $P$ during training (where $P$ is our dropout hyperparameter). This is a form of regularization, which is applied between layers. During inference and evaluation, the dropout layer will do nothing.\n",
    "\n",
    "\n",
    "\n",
    "You should also implement the forward function.\n",
    "The intended order of the layers is:\n",
    "\n",
    "- word embeddings -> dropout -> GRU -> dropout -> linear\n",
    "\n",
    "**Note:** The forward function of the GRU layer will return two tensors as a tuple.\n",
    "Look [here](https://pytorch.org/docs/stable/generated/torch.nn.GRU.html) to find out which of them you should pass to the next layer.\n",
    "Remember that this is a tagging task, i.e., we want to classify all words in the sequence. \n",
    "\n",
    "There is no nonlinearity after the final linear layer. We will use nn.CrossEntropyLoss() later, which has a built-in softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "eWSO5n7kh6KP"
   },
   "outputs": [],
   "source": [
    "class GRUTagger(nn.Module):\n",
    "  def __init__(self, config):\n",
    "    super().__init__()\n",
    "    self.embedding1 = nn.Embedding(embedding_dim=config['embedding_dim'],num_embedding=config['num_embedding'])\n",
    "    self.gru1 = nn.GRU(hidden_size=2*config['hidden_size'], batch_first=True)\n",
    "    self.linear1 = nn.Linear(config['num_embedding'],config['num_classes'])\n",
    "    self.dropout1 = nn.Dropout(config['dropout'])\n",
    "    # TODO: Instantiate the layers, using the hyperparameters in the config dictionary.\n",
    "\n",
    "  def forward(self, inputs):\n",
    "    self.gru(inputs)\n",
    "    # TODO: Complete the forward function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhQaKUk09yS3"
   },
   "source": [
    "# Training\n",
    "\n",
    "## Single step\n",
    "The do_step function does the forward pass and (if necessary) backward pass and gradient update on a single batch.\n",
    "\n",
    "Remember that we padded our inputs with zeros.\n",
    "Predicting the tag of a padding token is trivial (!PAD! is always tagged as !PAD!), so if we include the padded positions in our loss and accuracy calculations, we will overestimate our performance.\n",
    "Therefore, we must get rid of them.\n",
    "To do this, you should define a boolean mask, which is False in all positions where the inputs are padded (0) and True everywhere else."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "jS97bmxwl1lv"
   },
   "outputs": [],
   "source": [
    "def do_step(model, inputs, targets, optimizer = None):\n",
    "  device = next(model.parameters()).device\n",
    "  inputs, targets = inputs.to(device=device), targets.to(device=device)\n",
    "  logits = model(inputs)\n",
    "  pad_mask = !inputs.not_equal(0) # TODO: Define the boolean mask\n",
    "  \n",
    "  loss_func = nn.CrossEntropyLoss() # CrossEntropyLoss = Softmax and NLL\n",
    "  loss = loss_func(logits[pad_mask], targets[pad_mask])\n",
    "\n",
    "  if optimizer is not None:\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "  # Note: We do not average the accuracy at this point.\n",
    "  # That is because there may be different numbers of non-pad tokens per batch,\n",
    "  # so if we average within batches first, we will weight batches differently.\n",
    "  # Instead, we return the number of correct predictions and the number\n",
    "  # of non-padding tokens, so we can calculate the unweighted accuracy later.\n",
    "\n",
    "  correct = (logits[pad_mask].argmax(-1) == targets[pad_mask])\n",
    "  num_correct = correct.to(device='cpu', dtype=torch.float32).sum().detach()\n",
    "  num_non_pad = pad_mask.to(device='cpu', dtype=torch.float32).sum().detach()\n",
    "  loss_sum = (loss * inputs.shape[0]).to(device='cpu').detach()\n",
    "\n",
    "  return loss_sum.numpy(), num_correct.numpy(), num_non_pad.numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7QQS8j4b1KbD"
   },
   "source": [
    "## Single epoch\n",
    "\n",
    "The do_epoch function does a single epoch and calculates the overall loss and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "KdWOYyz3wZei"
   },
   "outputs": [],
   "source": [
    "def do_epoch(model, dataloader, optimizer=None):\n",
    "  if optimizer is None:\n",
    "    model.eval() # this disables dropout during evaluation\n",
    "  else:\n",
    "    model.train()\n",
    "    \n",
    "  total_loss, total_correct, total_non_pad = 0.0, 0.0, 0.0\n",
    "  for inputs, targets in dataloader:\n",
    "    loss, num_correct, num_non_pad = do_step(model, inputs, targets, \n",
    "                                             optimizer=optimizer)\n",
    "    total_loss += loss\n",
    "    total_correct += num_correct\n",
    "    total_non_pad += num_non_pad\n",
    "  \n",
    "  return total_loss / len(dataloader.dataset), total_correct / total_non_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7Cn_1HUw1Zd3"
   },
   "source": [
    "# Training with early stopping\n",
    "\n",
    "The do_training_with_early_stopping function trains the model on for a specified number of epochs.\n",
    "It also keeps track of the model's dev set (validation set) accuracy.\n",
    "\n",
    "We tune the number of epochs by doing early stopping, where we return the model from the best epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "0_tjh7XM0Q3j"
   },
   "outputs": [],
   "source": [
    "def do_training_with_early_stopping(model, \n",
    "                                    optimizer, \n",
    "                                    dataloaders, \n",
    "                                    epochs, \n",
    "                                    patience):\n",
    "  best_model, best_epoch, best_dev_acc = None, 0, -np.inf\n",
    "\n",
    "  for epoch in trange(epochs):\n",
    "    _, _ = do_epoch(model, dataloaders['train'], optimizer=optimizer)\n",
    "    _, dev_acc = do_epoch(model, dataloaders['dev'], optimizer=None)\n",
    "\n",
    "    if dev_acc > best_dev_acc:\n",
    "      best_epoch = epoch\n",
    "      best_dev_acc = dev_acc\n",
    "      best_model = copy.deepcopy(model) \n",
    "      # We want to return the model from the best epoch, not from the last epoch\n",
    "\n",
    "    if epoch - best_epoch > patience:\n",
    "      break\n",
    "\n",
    "  return best_model, best_dev_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "or2A8LFEgD3d"
   },
   "source": [
    "# Hyperparameter search\n",
    "\n",
    "## Ranges\n",
    "\n",
    "Here you should define ranges (permissible values) for the hyperparameters, by filling in the list for each hyperparameter key in the CONFIG_RANGES dictionary. There should be about 3 or 4 values per key.\n",
    "\n",
    "**Important:** The optimizers are strings. Learning rate, weight decay and dropout are floats, the others are integers. Remember that dropout is a probability.\n",
    "\n",
    "**Note:** Use your domain knowledge to choose a sensible range for each hyperparameter. \n",
    "Keep in mind that we are dealing with a small dataset, and that overfitting will be an issue. To get an idea of what sensible learning rates and weight decays are, look at the defaults [here](https://pytorch.org/docs/stable/optim.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "Fnw8bmyatGdN"
   },
   "outputs": [],
   "source": [
    "# TODO: Fill in some reasonable hyperparameter values for each hyperparameter\n",
    "# There should be 3-4 values per key\n",
    "\n",
    "CONFIG_RANGES = {'embedding_dim': [],\n",
    "                 'hidden_size': [],\n",
    "                 'batch_size': [],\n",
    "                 'optimizer': [],\n",
    "                 'learning_rate': [],\n",
    "                 'weight_decay': [],\n",
    "                 'dropout': []}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YkELn2ztFyiR"
   },
   "source": [
    "## Search strategy\n",
    "\n",
    "Now, we implement our hyperparameter search strategy.\n",
    "We will do simple uniform sampling.\n",
    "\n",
    "The input to the sample_configs function consists of a budget (number of configurations that you will sample), and a dictionary of ranges, e.g.:\n",
    "```\n",
    "{\n",
    "  'embedding_dim': [16, 32, ...], \n",
    "  'optimizer': ['adamw', 'sgd'...], \n",
    "  ...\n",
    "}\n",
    "```\n",
    "\n",
    "The output is a list of config dictionaries. \n",
    "For every config dictionary, pick one random value per hyperparameter, e.g.:\n",
    "```\n",
    "[\n",
    "  {'embedding_dim': 32, 'optimizer': 'adamw', ...}, \n",
    "  {'embedding_dim': 16, 'optimizer': 'sgd', ...}, \n",
    "  {'embedding_dim': 64, 'optimizer': 'sgd', ...},\n",
    "  ...\n",
    "]\n",
    "```\n",
    "\n",
    "The length of the output list corresponds to the budget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "JohqoJOF12Hh"
   },
   "outputs": [],
   "source": [
    "def sample_configs(budget, config_ranges):\n",
    "  tmp = []\n",
    "  for i in range(budget):\n",
    "    tmp.append(config_ranges)\n",
    "  return tmp # TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HrEEUfbjcjoZ"
   },
   "source": [
    "# Hyperparameter optimization\n",
    "\n",
    "## Optimization run\n",
    "\n",
    "This function does a single hyperparameter optimization run. It takes as input a hyperparameter config dictionary and:\n",
    "- instantiates a model, according to the config\n",
    "- instantiates an optimizer, according to the config (TODO!)\n",
    "- instantiates data loaders with the specified batch size\n",
    "- trains the model and returns the best dev set accuracy and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "sd15stJwwdVg"
   },
   "outputs": [],
   "source": [
    "def do_optimization_run(config, datasets):\n",
    "  assert not 'test' in datasets\n",
    "  \n",
    "  OPTIMIZER_CLASSES = {'adamw': optim.AdamW, \n",
    "                       'adam': optim.Adam,\n",
    "                       'adagrad': optim.Adagrad,\n",
    "                       'rmsprop': optim.RMSprop,\n",
    "                       'adadelta': optim.Adadelta,\n",
    "                       'sgd': optim.SGD}\n",
    "\n",
    "  model = GRUTagger(config)\n",
    "  if torch.cuda.is_available():\n",
    "    model = model.to(device='cuda')\n",
    "\n",
    "  # TODO: Instantiate the selected optimizer with the selected learning rate and\n",
    "  # weight decay, according to the config dictionary\n",
    "  optimizer = OPTIMIZER_CLASS[config['optimizer']]\n",
    "  \n",
    "  dataloaders = {dsetname: data.DataLoader(datasets[dsetname], \n",
    "                                           batch_size=config['batch_size'],\n",
    "                                           shuffle=dsetname=='train')\n",
    "                 for dsetname in datasets}\n",
    "  \n",
    "  return do_training_with_early_stopping(model, \n",
    "                                         optimizer, \n",
    "                                         dataloaders, \n",
    "                                         epochs=config['epochs'],\n",
    "                                         patience=config['patience'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyXRAYjNAwmA"
   },
   "source": [
    "# Outer loop\n",
    "\n",
    "This is the outer loop of the hyperparameter optimization:\n",
    "We loop over the configurations, store their development set accuracies, and remember which model performed best.\n",
    "You should implement the logic that decides which model we keep as the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "WnAF0X1IxUly"
   },
   "outputs": [],
   "source": [
    "def do_hyperparameter_optimization(configs, datasets):\n",
    "  dev_accs = []\n",
    "  best_model = None\n",
    "  for i, config in enumerate(configs):\n",
    "    print(f'Config {i+1}/{len(configs)}:', \n",
    "          ' '.join(f'{key}:{val}' for key, val in config.items()))\n",
    "\n",
    "    # we put a try-catch around the optimization runs\n",
    "    # this is in case of GPU memory errors or similar issues\n",
    "    # if every run fails and you end up with best_model=None, \n",
    "    # assume that something is wrong with your code\n",
    "    try:\n",
    "      model, dev_acc = do_optimization_run(config, datasets)\n",
    "    except Exception as e:\n",
    "      print('Unsuccessful run threw exception:', e)\n",
    "      model, dev_acc = None, -np.inf\n",
    "\n",
    "    dev_accs.append(dev_acc)\n",
    "\n",
    "    if dev_acc>max(dev_accs): # TODO: Condition under which the current model is the new best model\n",
    "      best_model = model\n",
    "      print(f'New best dev acc: {dev_acc:.4}')\n",
    "\n",
    "  return best_model, dev_accs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DP03O1ePoebU"
   },
   "source": [
    "# Let's go!\n",
    "\n",
    "Feel free to increase the BUDGET parameter for a higher chance of finding a good configuration. A higher budget means that you will wait longer for the result.\n",
    "\n",
    "If you are on a GPU, every epoch will be faster, so you can afford a higher budget and a higher number of epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "TpDbvYMz1IfH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config 1/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 2/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 3/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 4/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 5/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 6/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 7/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 8/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 9/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 10/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 11/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 12/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 13/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 14/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n",
      "Config 15/15: embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Unsuccessful run threw exception: 'num_embedding'\n"
     ]
    }
   ],
   "source": [
    "BUDGET = 50 if torch.cuda.is_available() else 15\n",
    "\n",
    "configs = sample_configs(budget=BUDGET, config_ranges=CONFIG_RANGES)\n",
    "\n",
    "# some additional model and training parameters\n",
    "for config in configs:\n",
    "  config['num_classes'] = len(tag2idx)\n",
    "  config['num_embeddings'] = len(word2idx)\n",
    "  config['epochs'] = 250 if torch.cuda.is_available() else 75\n",
    "  config['patience'] = 25\n",
    "\n",
    "train_and_dev = {dsetname: datasets[dsetname] for dsetname in ('train', 'dev')}\n",
    "best_model, dev_accs = do_hyperparameter_optimization(configs, train_and_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X0wdS7odXn3J"
   },
   "source": [
    "Here is a ranking of all the configurations that we evaluated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "Vbl_frDVbeLI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configs ranked:\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n",
      "Dev acc: -inf; embedding_dim:[] hidden_size:[] batch_size:[] optimizer:[] learning_rate:[] weight_decay:[] dropout:[] num_classes:13 num_embeddings:1682 epochs:75 patience:25\n"
     ]
    }
   ],
   "source": [
    "print('Configs ranked:')\n",
    "for idx in np.argsort(dev_accs)[::-1]:\n",
    "  print(f'Dev acc: {dev_accs[idx]:.4};', \n",
    "        ' '.join(f'{key}:{val}' for key, val in configs[idx].items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9zNOplbJcjZL"
   },
   "source": [
    "# Evaluation on test set\n",
    "\n",
    "As a final step, we evaluate the model on the test set.\n",
    "We will also look at some predictions. \n",
    "\n",
    "Again, remember that there are lots of !UNK! words in the test set, which makes it hard to get a high accuracy.\n",
    "In a realistic setting, we would have used pretrained embeddings or a pretrained language model, such as BERT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Pkl02DvQchKI"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'eval'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_914755/652203560.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mtest_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtest_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Final test acc: {test_acc:.4}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_914755/3660730948.py\u001b[0m in \u001b[0;36mdo_epoch\u001b[0;34m(model, dataloader, optimizer)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdo_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# this disables dropout during evaluation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'eval'"
     ]
    }
   ],
   "source": [
    "test_loader = data.DataLoader(datasets['test'], batch_size=1)\n",
    "test_loss, test_acc = do_epoch(best_model, test_loader, optimizer=None)\n",
    "\n",
    "print(f'Final test acc: {test_acc:.4}')\n",
    "print()\n",
    "\n",
    "for (inputs, targets), _ in zip(test_loader, range(3)):\n",
    "  device = next(best_model.parameters()).device\n",
    "  inputs, targets = inputs.to(device), targets.to(device)\n",
    "  predictions = best_model(inputs).argmax(-1)\n",
    "  inputs, targets, predictions = [x.to('cpu').detach().numpy().squeeze(0) \\\n",
    "                                  for x in (inputs, targets, predictions)]\n",
    "\n",
    "  print('Target:')\n",
    "  print(' '.join(f'{idx2word[word]}|{idx2tag[tag]}' \n",
    "                for word, tag in zip(inputs, targets) if word != 0))\n",
    "  print('Prediction:')\n",
    "  print(' '.join(f'{idx2word[word]}|{idx2tag[tag]}' \n",
    "                for word, tag in zip(inputs, predictions) if word != 0))\n",
    "  print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-2tDR4DshTKJ"
   },
   "source": [
    "**TODO:** Write a short report (approx. 4 sentences) about how you optimized the hyperparameters in this experiment.\n",
    "Include: The absolute and relative sizes of the train, dev and test sets, your method of hyperparameter search, the number of hyperparameter configurations (budget), the metric by which you chose the best model, what the best configuration was, and the final dev and test set accuracy."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "exercise7_tagginghyper_todo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
